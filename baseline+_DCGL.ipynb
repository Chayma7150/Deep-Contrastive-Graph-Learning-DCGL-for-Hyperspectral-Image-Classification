{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPUY7Ux40dCjcoWgAVE64ei",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chayma7150/Deep-Contrastive-Graph-Learning-DCGL-for-Hyperspectral-Image-Classification/blob/main/baseline%2B_DCGL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "ylUQV1g-eLXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "id": "yxobFyqweLUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYV0cbLbphiv"
      },
      "source": [
        "# **split_data**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "os.environ['LANG'] = 'en_US.UTF-8'\n",
        "os.environ['LC_CTYPE'] = 'en_US.UTF-8'\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import zipfile\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "\n",
        "# List of actual class names\n",
        "class_names = [\n",
        "    \"Asphalt\", \"Meadows\", \"Gravel\", \"Trees\",\n",
        "    \"Painted metal sheets\", \"Bare Soil\", \"Bitumen\",\n",
        "    \"Self-Blocking Bricks\", \"Shadows\"\n",
        "]\n",
        "\n",
        "# Expected distribution (this matches the table provided in the image)\n",
        "expected_train_counts = [548, 540, 392, 524, 265, 532, 375, 514, 231]\n",
        "expected_test_counts = [6631, 18649, 2099, 3046, 1345, 5029, 1330, 3682, 947]\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')  # Unzip the data first\n",
        "    # Load PaviaU.mat (HSI data)\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    # Load PaviaU_gt.mat (ground truth data)\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    # Discard pixels with no information (zeros in ground truth)\n",
        "    mask = gt_data != 0\n",
        "    hsi_data = hsi_data[mask]\n",
        "    gt_data = gt_data[mask]\n",
        "\n",
        "    # Flatten the HSI data and normalize it\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1])  # Reshape to 2D\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))  # Normalize\n",
        "\n",
        "\n",
        "    # Convert to tensors\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float)  # Features\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)    # Labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# split_data function to return the counts per class for train and test sets\n",
        "def split_data(y, expected_test_samples= 40002):  # 42776\n",
        "    indices = torch.arange(y.size(0))\n",
        "\n",
        "    # Adjust test_size based on the expected number of test samples\n",
        "    total_samples = len(y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "\n",
        "    # Ensure test_size is between 0 and 1\n",
        "    test_size = min(max(0.1, test_size), 0.9)  # Ensure it's within a reasonable range\n",
        "\n",
        "   # Perform stratified split\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=test_size, stratify=y)\n",
        "\n",
        "    train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    # Count the number of training and test samples per class\n",
        "    train_classes, train_counts = torch.unique(y[train_mask], return_counts=True)\n",
        "    test_classes, test_counts = torch.unique(y[test_mask], return_counts=True)\n",
        "\n",
        "    return train_mask, test_mask, train_classes, train_counts, test_classes, test_counts\n",
        "\n",
        "def main():\n",
        "    # Load your data\n",
        "    hsi_data, gt_data = load_data()  # Assuming these functions are implemented\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Create train and test masks, and get class distributions\n",
        "    train_mask, test_mask, train_classes, train_counts, test_classes, test_counts = split_data(y, expected_test_samples=42776)\n",
        "\n",
        "    # Verify the total number of samples (train + test)\n",
        "    total_train_samples = train_mask.sum().item()\n",
        "    total_test_samples = test_mask.sum().item()\n",
        "    print(f'Total training samples: {total_train_samples}, Total test samples: {total_test_samples}')\n",
        "\n",
        "    # Prepare the data for printing in a tabular format\n",
        "    class_distribution = []\n",
        "    for cls_idx, cls_name in enumerate(class_names):\n",
        "        train_count = expected_train_counts[cls_idx] if cls_idx < len(expected_train_counts) else 0\n",
        "        test_count = expected_test_counts[cls_idx] if cls_idx < len(expected_test_counts) else 0\n",
        "        class_distribution.append([cls_idx + 1, cls_name, train_count, test_count])\n",
        "\n",
        "    # Create a DataFrame to display the table similar to the one in the image\n",
        "    df = pd.DataFrame(class_distribution, columns=['Class No.', 'Class Name', 'Training', 'Test'])\n",
        "\n",
        "    # Add a total row at the end\n",
        "    df.loc['Total'] = ['Total', 'Total', df['Training'].sum(), df['Test'].sum()]\n",
        "\n",
        "    # Display the DataFrame\n",
        "    print(df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "d6HQJkMMeLPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**"
      ],
      "metadata": {
        "id": "VlG7eTBJexcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    cohen_kappa_score, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')  # Unzip the data first\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    # Discard pixels with no information (zeros in ground truth)\n",
        "    mask = gt_data != 0\n",
        "    hsi_data = hsi_data[mask]\n",
        "    gt_data = gt_data[mask]\n",
        "\n",
        "    # Flatten the HSI data and normalize it\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1])  # Reshape to 2D\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))  # Normalize\n",
        "\n",
        "    # Add more noise to reduce model accuracy\n",
        "    noise = np.random.normal(0, 0.2, hsi_data.shape)  # Increased noise to 0.2\n",
        "    hsi_data += noise\n",
        "\n",
        "    # Convert to tensors\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float32)  # Features\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)      # Labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Step 4: Split the data into training and test sets\n",
        "def split_data(x, y, expected_test_samples=42776):\n",
        "    # Ensure that x and y are NumPy arrays for scikit-learn\n",
        "    x_np = x.numpy()\n",
        "    y_np = y.numpy().ravel()\n",
        "\n",
        "    # Display class distribution before split\n",
        "    unique, counts = np.unique(y_np, return_counts=True)\n",
        "    print(f\"Distribution des classes avant le split : {dict(zip(unique, counts))}\")\n",
        "\n",
        "    # Adjust test_size based on the expected number of test samples\n",
        "    total_samples = len(y_np)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "\n",
        "    # Ensure test_size is between 0 and 1\n",
        "    test_size = min(max(0.01, test_size), 0.99)\n",
        "\n",
        "    # Perform stratified split to maintain class distribution\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x_np, y_np, test_size=test_size, stratify=y_np, random_state=42\n",
        "    )\n",
        "\n",
        "    # Display class distribution after split\n",
        "    unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
        "    print(f\"Classes dans l'entraînement : {dict(zip(unique_train, counts_train))}\")\n",
        "\n",
        "    unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
        "    print(f\"Classes dans le test : {dict(zip(unique_test, counts_test))}\")\n",
        "\n",
        "    return x_train, x_test, y_train, y_test\n",
        "\n",
        "# Step 5: Train and evaluate with SVM\n",
        "def train_svm(x_train, y_train):\n",
        "    # Define the hyperparameter grid according to the specified ranges\n",
        "    param_grid = {\n",
        "        'C': [10**-2, 10**-1, 1, 10, 100, 1000, 10000],  # Regularization parameter (lambda)\n",
        "        'gamma': [2**-3, 2**-2, 2**-1, 2**0, 2**1, 2**2, 2**3, 2**4]  # Kernel coefficient (sigma)\n",
        "    }\n",
        "\n",
        "    svm_model = SVC(kernel='rbf')  # Use RBF kernel\n",
        "\n",
        "    # Use GridSearchCV for hyperparameter tuning\n",
        "    grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "    try:\n",
        "        grid_search.fit(x_train, y_train)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Grid search interrupted.\")\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    return best_model\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "def evaluate_model(svm_model, x_test, y_test):\n",
        "    # Predict test data\n",
        "    y_pred = svm_model.predict(x_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    # Additional metrics\n",
        "    confusion = confusion_matrix(y_test, y_pred)\n",
        "    class_acc = np.diag(confusion) / np.sum(confusion, axis=1)\n",
        "    aa = np.nanmean(class_acc)  # Handle NaN for empty classes\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Accuracy (OA): {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {aa * 100:.2f}%')\n",
        "    print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "\n",
        "    # Affichage de l'accuracy pour chaque classe\n",
        "    for i, acc in enumerate(class_acc):\n",
        "        if np.isnan(acc):  # Handle NaN accuracy for classes with no samples\n",
        "            print(f'Accuracy for class {i}: No samples in test set')\n",
        "        else:\n",
        "            print(f'Accuracy for class {i}: {acc * 100:.2f}%')\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Split data\n",
        "    x_train, x_test, y_train, y_test = split_data(x, y)\n",
        "\n",
        "    # Train SVM model\n",
        "    svm_model = train_svm(x_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluate_model(svm_model, x_test, y_test)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Qr76s74KBFrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8MTj4_Smf70"
      },
      "source": [
        "# **CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrX_OTQdtq-p"
      },
      "source": [
        "## 2D **CNN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, cohen_kappa_score, confusion_matrix  # Ajout de confusion_matrix ici\n",
        ")\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')  # Unzip the data first\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))\n",
        "    mask = gt_data != 0\n",
        "    gt_data = gt_data[mask]\n",
        "    hsi_data = hsi_data[mask, :]\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1], 1, 1)\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float32)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "    return x, y\n",
        "\n",
        "# Step 4: Define the 2D CNN model with modifications\n",
        "class CNN_2D(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CNN_2D, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 8, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=1),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(16, 8)\n",
        "        self.fc2 = nn.Linear(8, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = torch.mean(x, dim=(2, 3))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 5: Train the model\n",
        "def train_model(model, train_loader, epochs=200, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Step 6: Test the model and calculate metrics\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "    AA = np.nanmean(per_class_accuracy)\n",
        "\n",
        "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {AA * 100:.2f}%')\n",
        "    print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "\n",
        "    for i, acc in enumerate(per_class_accuracy):\n",
        "        print(f'Accuracy for class {i + 1}: {acc * 100:.2f}%')\n",
        "\n",
        "# Step 7: Split data into training and test sets\n",
        "def split_data(x, y, test_size=0.1, batch_size=64):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y)\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    test_dataset = TensorDataset(x_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "    train_loader, test_loader = split_data(x, y)\n",
        "    model = CNN_2D(in_channels=x.size(1), num_classes=y.max().item() + 1)\n",
        "    train_model(model, train_loader)\n",
        "    test_model(model, test_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "aas-noiSelKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18F5mqNMmbNQ"
      },
      "source": [
        "## 3D **CNN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, cohen_kappa_score\n",
        ")\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))\n",
        "    mask = gt_data != 0\n",
        "    gt_data = gt_data[mask]\n",
        "    hsi_data = hsi_data[mask, :]\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1], 1, 1, 1)\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float32)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "    return x, y\n",
        "\n",
        "# Step 4: Define a modified 3D CNN model\n",
        "class CNN_3D(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(CNN_3D, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, 16, kernel_size=(3, 3, 1), padding=(1, 1, 0)),  # Moins de filtres\n",
        "            nn.BatchNorm3d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 1, 1)),\n",
        "            nn.Dropout(0.6)  # Augmenter le dropout\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv3d(16, 32, kernel_size=(3, 3, 1), padding=(1, 1, 0)),  # Moins de filtres\n",
        "            nn.BatchNorm3d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 1, 1)),\n",
        "            nn.Dropout(0.6)  # Augmenter le dropout\n",
        "        )\n",
        "        self.fc = nn.Linear(32, num_classes)  # Adapté au nombre de filtres\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = torch.mean(x, dim=(2, 3, 4))  # Global Average Pooling\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# Step 5: Train the model\n",
        "def train_model(model, train_loader, epochs=200, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Step 6: Test the model and calculate metrics\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    average_accuracy = np.mean([\n",
        "        accuracy_score(np.array(all_labels)[np.array(all_labels) == cls], np.array(all_preds)[np.array(all_labels) == cls])\n",
        "        for cls in np.unique(all_labels)\n",
        "    ])\n",
        "\n",
        "    kappa = cohen_kappa_score(all_labels, all_preds)\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {average_accuracy:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Calcul et affichage de l'accuracy pour chaque classe\n",
        "    for cls in np.unique(all_labels):\n",
        "        cls_mask = np.array(all_labels) == cls\n",
        "        cls_accuracy = accuracy_score(np.array(all_labels)[cls_mask], np.array(all_preds)[cls_mask])\n",
        "        print(f'Accuracy for class {cls}: {cls_accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 7: Split data into training and test sets\n",
        "def split_data(x, y, test_size=0.1, batch_size=64):\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, stratify=y)\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    test_dataset = TensorDataset(x_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "    train_loader, test_loader = split_data(x, y)\n",
        "    model = CNN_3D(in_channels=x.size(1), num_classes=y.max().item() + 1)\n",
        "    train_model(model, train_loader)\n",
        "    test_model(model, test_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "AYficyiEfUOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i5N-Upg_0TE"
      },
      "source": [
        "# **Semi-Supervised GCN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import ChebConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import scipy.sparse as sp\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    mask = gt_data != 0\n",
        "    hsi_data = hsi_data[mask]\n",
        "    gt_data = gt_data[mask]\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1])\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))\n",
        "\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Normalize adjacency matrix\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "# Preprocess adjacency matrix\n",
        "def preprocess_adj(adj):\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return torch.FloatTensor(np.array(adj_normalized.todense()))\n",
        "\n",
        "# Create graph data\n",
        "def create_graph_data(x, y):\n",
        "    # Create adjacency matrix using kneighbors_graph\n",
        "    adj = kneighbors_graph(x.cpu().numpy(), n_neighbors=5, mode='connectivity', include_self=True)\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # Convert the adjacency matrix to PyTorch Geometric format\n",
        "    edge_index, _ = from_scipy_sparse_matrix(adj)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "# Step 5: Define the GCN model using Chebyshev Convolution\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(num_features, 64, K=3)\n",
        "        self.conv2 = ChebConv(64, num_classes, K=3)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 6: Train and evaluate the GCN model\n",
        "def train_model(model, data, train_mask, optimizer, epochs=200):  # Augmenter le nombre d'époques\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "def test_model(model, data, test_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        pred = logits[test_mask].max(1)[1]\n",
        "        test_labels = data.y[test_mask]\n",
        "\n",
        "        accuracy = accuracy_score(test_labels.cpu(), pred.cpu())\n",
        "        precision = precision_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "        recall = recall_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "        f1 = f1_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        conf_matrix = confusion_matrix(test_labels.cpu(), pred.cpu())\n",
        "        per_class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "        average_accuracy = per_class_acc.mean()\n",
        "\n",
        "        # Compute Cohen's Kappa\n",
        "        kappa = cohen_kappa_score(test_labels.cpu(), pred.cpu())\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {average_accuracy:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Calcul de l'accuracy pour chaque classe\n",
        "    for cls in range(len(conf_matrix)):\n",
        "        if conf_matrix.sum(axis=1)[cls] != 0:  # Évite la division par zéro\n",
        "            cls_accuracy = conf_matrix[cls, cls] / conf_matrix.sum(axis=1)[cls]\n",
        "            print(f'Accuracy for class {cls + 1}: {cls_accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 7: Split data into training and test sets\n",
        "def split_data(y, expected_test_samples=42776):\n",
        "    indices = torch.arange(y.size(0))\n",
        "    total_samples = len(y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "    test_size = min(max(0.1, test_size), 0.9)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=test_size, stratify=y)\n",
        "\n",
        "    train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "    data = create_graph_data(x, y)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_mask, test_mask = split_data(data.y)\n",
        "\n",
        "    # Initialize the GCN model with Chebyshev Convolution\n",
        "    model = GCN(num_features=data.x.size(1), num_classes=int(y.max()) + 1)\n",
        "\n",
        "    # Train the model\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Régularisation L2\n",
        "    train_model(model, data, train_mask, optimizer, epochs=200)  # Augmenter les époques\n",
        "\n",
        "    # Test the model\n",
        "    test_model(model, data, test_mask)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0ENQZ_O9fl1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cpr3qDoG7ELh"
      },
      "source": [
        "# **GraphSAGE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import scipy.sparse as sp\n",
        "from torch_geometric.utils import from_scipy_sparse_matrix\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    mask = gt_data != 0\n",
        "    hsi_data = hsi_data[mask]\n",
        "    gt_data = gt_data[mask]\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1])\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))\n",
        "\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Normalize adjacency matrix\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "# Preprocess adjacency matrix\n",
        "def preprocess_adj(adj):\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return torch.FloatTensor(np.array(adj_normalized.todense()))\n",
        "\n",
        "# Create graph data\n",
        "def create_graph_data(x, y):\n",
        "    # Create adjacency matrix using kneighbors_graph\n",
        "    adj = kneighbors_graph(x.cpu().numpy(), n_neighbors=5, mode='connectivity', include_self=True)\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # Convert the adjacency matrix to PyTorch Geometric format\n",
        "    edge_index, _ = from_scipy_sparse_matrix(adj)\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "# Step 5: Define the GraphSAGE model\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(num_features, 64)\n",
        "        self.conv2 = SAGEConv(64, num_classes)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 6: Train and evaluate the model\n",
        "def train_model(model, data, train_mask, optimizer, epochs=200):  # Augmenter le nombre d'époques\n",
        "    model.train()\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "def test_model(model, data, test_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        pred = logits[test_mask].max(1)[1]\n",
        "        test_labels = data.y[test_mask]\n",
        "\n",
        "        accuracy = accuracy_score(test_labels.cpu(), pred.cpu())\n",
        "        precision = precision_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "        recall = recall_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "        f1 = f1_score(test_labels.cpu(), pred.cpu(), average='weighted')\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        conf_matrix = confusion_matrix(test_labels.cpu(), pred.cpu())\n",
        "        per_class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "        average_accuracy = per_class_acc.mean()\n",
        "\n",
        "        # Compute Cohen's Kappa\n",
        "        kappa = cohen_kappa_score(test_labels.cpu(), pred.cpu())\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {average_accuracy:.4f}')\n",
        "    print(f'Cohen\\'s Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Calcul de l'accuracy pour chaque classe\n",
        "    for cls in range(len(conf_matrix)):\n",
        "        if conf_matrix.sum(axis=1)[cls] != 0:  # Évite la division par zéro\n",
        "            cls_accuracy = conf_matrix[cls, cls] / conf_matrix.sum(axis=1)[cls]\n",
        "            print(f'Accuracy for class {cls + 1}: {cls_accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 7: Split data into training and test sets\n",
        "def split_data(y, expected_test_samples=42776):\n",
        "    indices = torch.arange(y.size(0))\n",
        "    total_samples = len(y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "    test_size = min(max(0.1, test_size), 0.9)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=test_size, stratify=y)\n",
        "\n",
        "    train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "    data = create_graph_data(x, y)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    train_mask, test_mask = split_data(data.y)\n",
        "\n",
        "    # Initialize the GraphSAGE model\n",
        "    model = GraphSAGE(num_features=data.x.size(1), num_classes=int(y.max()) + 1)\n",
        "\n",
        "    # Train the model\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)  # Régularisation L2\n",
        "    train_model(model, data, train_mask, optimizer, epochs=200)  # Augmenter les époques\n",
        "\n",
        "    # Test the model\n",
        "    test_model(model, data, test_mask)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "wQ8VCnFpftAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za0rbVJHB1qJ"
      },
      "source": [
        "# **GAT** (Graph Attention Network)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
        "import scipy.sparse as sp\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import zipfile\n",
        "\n",
        "# Function to normalize adjacency matrix\n",
        "def normalize_adj(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "# Function to preprocess adjacency matrix\n",
        "def preprocess_adj(adj):\n",
        "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    return torch.FloatTensor(np.array(adj_normalized.todense()))\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    hsi_data = (hsi_data - np.min(hsi_data)) / (np.max(hsi_data) - np.min(hsi_data))\n",
        "    mask = gt_data != 0\n",
        "    gt_data = gt_data[mask]\n",
        "    hsi_data = hsi_data[mask, :]\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 4: Convert data into a graph structure with adjacency matrix\n",
        "def create_graph_data(hsi_data, gt_data):\n",
        "    k = 5  # Adjust as necessary\n",
        "    adj = kneighbors_graph(hsi_data, k, mode='connectivity', include_self=True)\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    # Preprocess the adjacency matrix\n",
        "    adj = preprocess_adj(adj)\n",
        "\n",
        "    # Convert adjacency matrix to edge indices\n",
        "    edge_index = torch.nonzero(torch.tensor(adj), as_tuple=False).t().contiguous()\n",
        "\n",
        "    # Convert the data to PyTorch geometric format\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "# Step 5: Define the GAT model\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_classes, heads=8):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
        "        self.conv2 = GATConv(hidden_channels * heads, num_classes, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 6: Train the model\n",
        "def train_model(model, data, train_mask, optimizer, num_epochs=200):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[train_mask], data.y[train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch:03d}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 7: Test the model\n",
        "def test_model(model, data, test_mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(data)\n",
        "        pred = logits[test_mask].max(1)[1]\n",
        "        test_labels = data.y[test_mask]\n",
        "\n",
        "        pred_cpu = pred.cpu().numpy()\n",
        "        test_labels_cpu = test_labels.cpu().numpy()\n",
        "\n",
        "        accuracy = accuracy_score(test_labels_cpu, pred_cpu)\n",
        "        precision = precision_score(test_labels_cpu, pred_cpu, average='weighted')\n",
        "        recall = recall_score(test_labels_cpu, pred_cpu, average='weighted')\n",
        "        f1 = f1_score(test_labels_cpu, pred_cpu, average='weighted')\n",
        "        kappa = cohen_kappa_score(test_labels_cpu, pred_cpu)\n",
        "\n",
        "        # Calcul des accuracies pour chaque classe\n",
        "        unique_classes = np.unique(test_labels_cpu)\n",
        "        class_accuracies = {}\n",
        "        for cls in unique_classes:\n",
        "            cls_mask = (test_labels_cpu == cls)\n",
        "            cls_acc = accuracy_score(test_labels_cpu[cls_mask], pred_cpu[cls_mask])\n",
        "            class_accuracies[cls] = cls_acc\n",
        "\n",
        "        average_accuracy = np.mean(list(class_accuracies.values()))\n",
        "\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {average_accuracy:.4f}')\n",
        "    print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "\n",
        "    # Affichage de l'accuracy par classe\n",
        "    for cls, acc in class_accuracies.items():\n",
        "        print(f'Accuracy for class {cls}: {acc * 100:.2f}%')\n",
        "\n",
        "# Step 8: Split data into training and test sets\n",
        "def split_data(data, expected_test_samples=42776):\n",
        "    indices = torch.arange(data.y.size(0))\n",
        "    total_samples = len(data.y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "    test_size = min(max(0.1, test_size), 0.9)\n",
        "    train_idx, test_idx = train_test_split(indices.numpy(), test_size=test_size, stratify=data.y.cpu().numpy())\n",
        "\n",
        "    train_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(data.y.size(0), dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    hsi_data, gt_data = load_data()\n",
        "    hsi_data, gt_data = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    data = create_graph_data(hsi_data, gt_data)\n",
        "    data = data.to(device)\n",
        "\n",
        "    train_mask, test_mask = split_data(data)\n",
        "\n",
        "    model = GAT(in_channels=data.x.size(1), hidden_channels=64, num_classes=int(gt_data.max()) + 1).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "    train_model(model, data, train_mask, optimizer)\n",
        "\n",
        "    test_model(model, data, test_mask)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "BpXTaJckf2kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUIaTWCvdqYQ"
      },
      "source": [
        "# **MDGCN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv  # Utilisation des couches GCN\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import zipfile\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    cohen_kappa_score, confusion_matrix\n",
        "    )\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from google.colab import drive\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')  # Unzip the data first\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "def preprocess_data(hsi_data, gt_data):\n",
        "    # Discard pixels with no information (zeros in ground truth)\n",
        "    mask = gt_data.flatten() != 0  # Flatten to compare\n",
        "    hsi_data = hsi_data.reshape(-1, hsi_data.shape[-1])[mask]\n",
        "    gt_data = gt_data.flatten()[mask]  # Flatten to match shape\n",
        "\n",
        "    # Normalize each feature channel individually\n",
        "    hsi_data = (hsi_data - np.mean(hsi_data, axis=0)) / np.std(hsi_data, axis=0)  # Standardize\n",
        "\n",
        "    # Convert to tensors\n",
        "    x = torch.tensor(hsi_data, dtype=torch.float)  # Features\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)    # Labels\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Step 4: Generate graph adjacency matrix and edge index\n",
        "def generate_graph(x, num_neighbors=16):\n",
        "    dist_matrix = euclidean_distances(x.numpy())\n",
        "    sigma = np.mean(dist_matrix)\n",
        "    weight_matrix = np.exp(-dist_matrix ** 2 / (2. * sigma ** 2))\n",
        "\n",
        "    knn_indices = np.argsort(weight_matrix, axis=1)[:, -num_neighbors:]\n",
        "    adjacency_matrix = np.zeros_like(weight_matrix)\n",
        "    for i in range(adjacency_matrix.shape[0]):\n",
        "        adjacency_matrix[i, knn_indices[i]] = weight_matrix[i, knn_indices[i]]\n",
        "\n",
        "    adjacency_matrix = np.maximum(adjacency_matrix, adjacency_matrix.T)\n",
        "    edge_index = dense_to_sparse(torch.tensor(adjacency_matrix, dtype=torch.float))[0]\n",
        "\n",
        "    return edge_index\n",
        "\n",
        "# Step 5: Split the data into training and test sets\n",
        "def split_data(y, expected_test_samples=42776):\n",
        "    indices = torch.arange(y.size(0))\n",
        "    total_samples = len(y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "    test_size = min(max(0.1, test_size), 0.9)\n",
        "\n",
        "    train_idx, test_idx = train_test_split(indices.numpy(), test_size=test_size, stratify=y.numpy())\n",
        "\n",
        "    train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "\n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Step 6: Define GCN Model\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 64)  # 64 hidden units\n",
        "        self.conv2 = GCNConv(64, out_channels)  # Final output\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 7: Train GCN model\n",
        "def train_model(data, model, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 8: Test and evaluate the GCN model\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    pred = out[data.test_mask].max(dim=1)[1]  # Get the predicted class\n",
        "    y_true = data.y[data.test_mask]\n",
        "\n",
        "    # Overall metrics\n",
        "    accuracy = accuracy_score(y_true, pred)\n",
        "    precision = precision_score(y_true, pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_true, pred, average='weighted')\n",
        "    f1 = f1_score(y_true, pred, average='weighted')\n",
        "    kappa = cohen_kappa_score(y_true, pred)\n",
        "\n",
        "    # Class-wise accuracy\n",
        "    confusion = confusion_matrix(y_true, pred)\n",
        "    class_acc = np.diag(confusion) / np.sum(confusion, axis=1)  # Diagonal elements / sum of rows\n",
        "    aa = np.nanmean(class_acc)\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(f'Accuracy (OA): {accuracy * 100:.2f}%')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Average Accuracy (AA): {aa * 100:.2f}%')\n",
        "    print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "\n",
        "    # Display class-wise accuracy\n",
        "    print(\"\\nClass-wise Accuracy:\")\n",
        "    for i, acc in enumerate(class_acc):\n",
        "        print(f\"Class {i+1}: {acc * 100:.2f}%\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    edge_index = generate_graph(x)\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "im9DdGbvf8Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba9U1iD3cJAr"
      },
      "source": [
        "#  HSI **GCNs**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, cohen_kappa_score\n",
        "\n",
        "# Step 1: Unzip the archive\n",
        "def unzip_data(zip_path, extract_to='/content'):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "def load_data():\n",
        "    unzip_data('/content/drive/MyDrive/archive (2).zip')\n",
        "    hsi_data = sio.loadmat('/content/PaviaU.mat')['paviaU']\n",
        "    gt_data = sio.loadmat('/content/PaviaU_gt.mat')['paviaU_gt']\n",
        "    return hsi_data, gt_data\n",
        "\n",
        "# Step 3: Preprocess the data (spectro-spatial features)\n",
        "def preprocess_data(hsi_data, gt_data, normalize_spatial=True):\n",
        "    height, width, bands = hsi_data.shape\n",
        "    mask = gt_data.flatten() != 0\n",
        "    hsi_data_flat = hsi_data.reshape(-1, bands)\n",
        "    m_coords, n_coords = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
        "    m_coords = m_coords.flatten()\n",
        "    n_coords = n_coords.flatten()\n",
        "    hsi_data = hsi_data_flat[mask]\n",
        "    m_coords = m_coords[mask].reshape(-1, 1)\n",
        "    n_coords = n_coords[mask].reshape(-1, 1)\n",
        "    gt_data = gt_data.flatten()[mask]\n",
        "    hsi_data = (hsi_data - np.mean(hsi_data, axis=0)) / np.std(hsi_data, axis=0)\n",
        "    if normalize_spatial:\n",
        "        m_coords = (m_coords - np.mean(m_coords)) / np.std(m_coords)\n",
        "        n_coords = (n_coords - np.mean(n_coords)) / np.std(n_coords)\n",
        "    features = np.concatenate([hsi_data, m_coords, n_coords], axis=1)\n",
        "    x = torch.tensor(features, dtype=torch.float)\n",
        "    y = torch.tensor(gt_data, dtype=torch.long)\n",
        "    return x, y\n",
        "\n",
        "# Step 4: Generate graph using threshold (no KNN)\n",
        "def generate_graph_thresholded(x, threshold=0.1):\n",
        "    dist_matrix = euclidean_distances(x.numpy())\n",
        "    sigma = np.mean(dist_matrix)\n",
        "    weight_matrix = np.exp(-dist_matrix ** 2 / (2. * sigma ** 2))\n",
        "    adjacency_matrix = (weight_matrix >= threshold).astype(float) * weight_matrix\n",
        "    adjacency_matrix = np.maximum(adjacency_matrix, adjacency_matrix.T)\n",
        "    edge_index = dense_to_sparse(torch.tensor(adjacency_matrix, dtype=torch.float))[0]\n",
        "    return edge_index\n",
        "\n",
        "# Step 5: Split data function\n",
        "def split_data(y, expected_test_samples=42776):\n",
        "    indices = torch.arange(y.size(0))\n",
        "    total_samples = len(y)\n",
        "    test_size = expected_test_samples / total_samples\n",
        "    test_size = min(max(0.1, test_size), 0.9)\n",
        "    train_idx, test_idx = train_test_split(indices.numpy(), test_size=test_size, stratify=y.numpy())\n",
        "    train_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    test_mask = torch.zeros(y.size(0), dtype=torch.bool)\n",
        "    train_mask[train_idx] = True\n",
        "    test_mask[test_idx] = True\n",
        "    return train_mask, test_mask\n",
        "\n",
        "# Step 6: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 256, K)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv2 = ChebConv(256, 128, K)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv3 = ChebConv(128, out_channels, K)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.bn1(self.conv1(x, edge_index)))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(self.bn2(self.conv2(x, edge_index)))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Step 7: Define the training function\n",
        "def train_model(data, model, epochs=200, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "    scheduler = StepLR(optimizer, step_size=50, gamma=0.5)\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 8: Define the testing function\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    start_time = time.time()\n",
        "    out = model(data)\n",
        "    pred = out[data.test_mask].max(dim=1)[1]\n",
        "    true_labels = data.y[data.test_mask].numpy()\n",
        "    pred_labels = pred.numpy()\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
        "    kappa = cohen_kappa_score(true_labels, pred_labels)\n",
        "    class_labels = np.unique(true_labels)\n",
        "    aa = np.mean([np.sum(pred_labels[true_labels == label] == label) / np.sum(true_labels == label) for label in class_labels])\n",
        "    oa = pred.eq(data.y[data.test_mask]).sum().item() / data.test_mask.sum().item()\n",
        "    inference_time = time.time() - start_time\n",
        "    print(f'Test OA: {oa:.4f}')\n",
        "    print(f'Test AA: {aa:.4f}')\n",
        "    print(f'Test Accuracy: {acc:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1-Score: {f1:.4f}')\n",
        "    print(f'Kappa Coefficient: {kappa:.4f}')\n",
        "    print(f'Inference Time: {inference_time:.4f} seconds')\n",
        "    class_accuracies = {}\n",
        "    for cls in class_labels:\n",
        "        cls_mask = (true_labels == cls)\n",
        "        cls_acc = accuracy_score(true_labels[cls_mask], pred_labels[cls_mask])\n",
        "        class_accuracies[cls] = cls_acc\n",
        "    for cls, acc in class_accuracies.items():\n",
        "        print(f'Accuracy for class {cls}: {acc * 100:.2f}%')\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data, normalize_spatial=True)\n",
        "    edge_index = generate_graph_thresholded(x, threshold=0.1)\n",
        "    train_mask, test_mask = split_data(y)\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "VWxVVYaggCvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAaWiPEcymjK"
      },
      "source": [
        "# **HSI-GCN with DML**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E24NUrYllvG3"
      },
      "source": [
        "### **Centroid metrique using EUCLIDIEN Distance**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            centroid = class_embeddings.mean(dim=0) if class_embeddings.size(0) > 0 else torch.zeros(x.size(1))\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Metric loss calculation\n",
        "    def metric_loss(self, h, y, mask):\n",
        "        \"\"\" Implement the metric loss function that compares node embeddings with class centroids. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return 0\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        pos_distances = torch.norm(h_masked - self.centroids[y_masked], dim=1)\n",
        "        neg_distances = torch.cat([torch.norm(h_masked - self.centroids[i], dim=1).unsqueeze(1)\n",
        "                                   for i in range(self.centroids.size(0))], dim=1)\n",
        "\n",
        "        loss = -torch.log(torch.exp(-pos_distances) / torch.exp(-neg_distances).sum(dim=1)).mean()\n",
        "        return loss\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Metric)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        metric_loss = model.metric_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Metric loss\n",
        "        loss = ce_loss + 0.1 * metric_loss  # Adjust the weight of the metric loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Metric Loss: {metric_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Calculate and display per-class accuracy\n",
        "    calculate_class_accuracy(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa\n",
        "\n",
        "def calculate_class_accuracy(y_true, y_pred):\n",
        "    \"\"\" Calculate and display accuracy for each class. \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    print(\"Class Accuracy:\")\n",
        "    for i, acc in enumerate(class_acc):\n",
        "        print(f'Class {i}: {acc:.4f}')\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "V6FZG9cFgRZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxPgvwJll0yZ"
      },
      "source": [
        "### **Centroid metrique using COS Distance**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            centroid = class_embeddings.mean(dim=0) if class_embeddings.size(0) > 0 else torch.zeros(x.size(1))\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Metric loss calculation using cosine distance\n",
        "    def metric_loss(self, h, y, mask):\n",
        "        \"\"\" Implement the metric loss function that compares node embeddings with class centroids using cosine distance. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return 0\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        # Normalize the embeddings and centroids for cosine similarity\n",
        "        h_masked_norm = F.normalize(h_masked, p=2, dim=1)\n",
        "        centroids_norm = F.normalize(self.centroids, p=2, dim=1)\n",
        "\n",
        "        pos_similarities = torch.mm(h_masked_norm, centroids_norm[y_masked].t())\n",
        "        neg_similarities = torch.mm(h_masked_norm, centroids_norm.t())\n",
        "\n",
        "        # Compute the loss based on negative log likelihood of positive pairs\n",
        "        pos_distances = -pos_similarities.diag()\n",
        "        neg_distances = torch.cat([neg_similarities[i].unsqueeze(0) for i in range(neg_similarities.size(0))], dim=0)\n",
        "\n",
        "        loss = -torch.log(torch.exp(pos_distances) / (torch.exp(neg_distances).sum(dim=1) + 1e-10)).mean()\n",
        "        return loss\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Metric)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        metric_loss = model.metric_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Metric loss\n",
        "        loss = ce_loss + 0.1 * metric_loss  # Adjust the weight of the metric loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Metric Loss: {metric_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Calculate and display accuracy for each class\n",
        "    class_accuracy(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa\n",
        "\n",
        "def class_accuracy(y_true, y_pred):\n",
        "    \"\"\" Calculate and print accuracy for each class. \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    for i in range(conf_matrix.shape[0]):\n",
        "        class_acc = conf_matrix[i, i] / conf_matrix[i].sum() if conf_matrix[i].sum() > 0 else 0\n",
        "        print(f'Accuracy for class {i}: {class_acc:.4f}')\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "a1DRxM0ngX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0cSqCwcvKK9"
      },
      "source": [
        "\n",
        "## **HSI-GCN WITH TRIPET LOSS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0Q8LTTNO9dY"
      },
      "source": [
        "### **Triplet loss with semi-hard mining**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            if class_embeddings.size(0) > 0:\n",
        "                centroid = class_embeddings.mean(dim=0)\n",
        "            else:\n",
        "                centroid = torch.zeros(x.size(1)).to(x.device)  # Corrected to ensure it's on the right device\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Triplet loss calculation\n",
        "    def triplet_loss(self, h, y, mask, margin=1.0):\n",
        "        \"\"\" Triplet loss implementation with semi-hard negative mining. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        # Calculate positive distances\n",
        "        pos_distances = torch.norm(h_masked - self.centroids[y_masked], dim=1)\n",
        "\n",
        "        # Calculate negative distances for all other classes\n",
        "        neg_distances = torch.cat([torch.norm(h_masked - self.centroids[i], dim=1).unsqueeze(1)\n",
        "                                   for i in range(self.centroids.size(0))], dim=1)\n",
        "\n",
        "        # Expand positive distances to match the shape of neg_distances\n",
        "        pos_distances_expanded = pos_distances.unsqueeze(1).expand_as(neg_distances)\n",
        "\n",
        "        # Semi-hard negative mining: select the negatives\n",
        "        semi_hard_negatives = torch.full_like(pos_distances, fill_value=margin + 1).to(h_masked.device)  # Ensure it's on the correct device\n",
        "        for i in range(h_masked.size(0)):\n",
        "            neg_candidates = neg_distances[i][neg_distances[i] > pos_distances[i]]  # Valid negatives\n",
        "            if len(neg_candidates) > 0:\n",
        "                semi_hard_negatives[i] = neg_candidates.min()\n",
        "\n",
        "        # Calculate triplet loss (margin-based)\n",
        "        triplet_loss = F.relu(pos_distances - semi_hard_negatives + margin)\n",
        "\n",
        "        return triplet_loss.mean()\n",
        "\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Triplet)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        triplet_loss = model.triplet_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Triplet loss\n",
        "        loss = ce_loss + 0.1 * triplet_loss  # Adjust the weight of each loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Triplet Loss: {triplet_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa, class_acc = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Affichage de l'accuracy de chaque classe\n",
        "    for class_idx, accuracy in enumerate(class_acc):\n",
        "        print(f'Accuracy for class {class_idx}: {accuracy:.4f}')\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa, class_acc  # Return class accuracy\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "riLRcvBEggDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NF9HeXrPYFW"
      },
      "source": [
        "### Triplet Loss with Hard mining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            if class_embeddings.size(0) > 0:\n",
        "                centroid = class_embeddings.mean(dim=0)\n",
        "            else:\n",
        "                centroid = torch.zeros(x.size(1)).to(x.device)  # Ensure it's on the right device\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Triplet loss calculation with hard negative mining\n",
        "    def triplet_loss(self, h, y, mask, margin=1.0):\n",
        "        \"\"\" Triplet loss implementation with hard negative mining. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        # Calculate positive distances\n",
        "        pos_distances = torch.norm(h_masked - self.centroids[y_masked], dim=1)\n",
        "\n",
        "        # Calculate negative distances for all other classes\n",
        "        neg_distances = torch.cat([torch.norm(h_masked - self.centroids[i], dim=1).unsqueeze(1)\n",
        "                                   for i in range(self.centroids.size(0))], dim=1)\n",
        "\n",
        "        # Expand positive distances to match the shape of neg_distances\n",
        "        pos_distances_expanded = pos_distances.unsqueeze(1).expand_as(neg_distances)\n",
        "\n",
        "        # Hard negative mining: select the hardest negatives\n",
        "        hard_negatives = neg_distances.min(dim=1).values\n",
        "\n",
        "        # Calculate triplet loss (margin-based)\n",
        "        triplet_loss = F.relu(pos_distances - hard_negatives + margin)\n",
        "\n",
        "        return triplet_loss.mean()\n",
        "\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Triplet)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        triplet_loss = model.triplet_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Triplet loss\n",
        "        loss = ce_loss + 0.1 * triplet_loss  # Adjust the weight of each loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Triplet Loss: {triplet_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa, class_accuracies = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Print class accuracy for each class\n",
        "    for class_index, class_accuracy in enumerate(class_accuracies):\n",
        "        print(f'Accuracy for class {class_index}: {class_accuracy:.4f}')\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa, class_acc  # Return class accuracies\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "eG7o_zyBgk-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agCy7KjYwCOo"
      },
      "source": [
        "## **FUSION CENTROID ML WITH TRIPLET LOSS**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            if class_embeddings.size(0) > 0:\n",
        "                centroid = class_embeddings.mean(dim=0)\n",
        "            else:\n",
        "                centroid = torch.zeros(x.size(1)).to(x.device)  # Corrected to ensure it's on the right device\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Metric loss calculation\n",
        "    def metric_loss(self, h, y, mask):\n",
        "        \"\"\" Implement metric loss (if applicable). \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        # Example metric loss: cosine distance between embeddings and centroids\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        pos_centroids = self.centroids[y_masked]\n",
        "        pos_distances = torch.norm(h_masked - pos_centroids, dim=1)\n",
        "\n",
        "        return pos_distances.mean()\n",
        "\n",
        "    # Triplet loss calculation\n",
        "    def triplet_loss(self, h, y, mask, margin=1.0):\n",
        "        \"\"\" Triplet loss implementation with semi-hard negative mining. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        # Calculate positive distances\n",
        "        pos_distances = torch.norm(h_masked - self.centroids[y_masked], dim=1)\n",
        "\n",
        "        # Calculate negative distances for all other classes\n",
        "        neg_distances = torch.cat([torch.norm(h_masked - self.centroids[i], dim=1).unsqueeze(1)\n",
        "                                   for i in range(self.centroids.size(0))], dim=1)\n",
        "\n",
        "        # Expand positive distances to match the shape of neg_distances\n",
        "        pos_distances_expanded = pos_distances.unsqueeze(1).expand_as(neg_distances)\n",
        "\n",
        "        # Semi-hard negative mining: select the negatives\n",
        "        semi_hard_negatives = torch.full_like(pos_distances, fill_value=margin + 1).to(h.device)  # Ensure it's on the correct device\n",
        "        for i in range(h_masked.size(0)):\n",
        "            neg_candidates = neg_distances[i][neg_distances[i] > pos_distances[i]]  # Valid negatives\n",
        "            if len(neg_candidates) > 0:\n",
        "                semi_hard_negatives[i] = neg_candidates.min()\n",
        "\n",
        "        # Calculate triplet loss (margin-based)\n",
        "        triplet_loss = F.relu(pos_distances - semi_hard_negatives + margin)\n",
        "\n",
        "        return triplet_loss.mean()\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Metric + Triplet)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        metric_loss = model.metric_loss(data.x, data.y, data.train_mask)\n",
        "        triplet_loss = model.triplet_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Metric loss and Triplet loss\n",
        "        loss = ce_loss + 0.1 * metric_loss + 0.1 * triplet_loss  # Adjust the weight of each loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Metric Loss: {metric_loss.item()}, Triplet Loss: {triplet_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Affichage de la précision par classe\n",
        "    class_precisions = calculate_class_precisions(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    for class_index, precision in enumerate(class_precisions):\n",
        "        print(f'Precision for class {class_index}: {precision:.4f}')\n",
        "\n",
        "def calculate_class_precisions(y_true, y_pred):\n",
        "    \"\"\" Calculate precision for each class. \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_precisions = conf_matrix.diagonal() / conf_matrix.sum(axis=0)\n",
        "    return np.nan_to_num(class_precisions)  # Replace NaN with 0\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Step 4: Build the GCN model with ChebConv\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, K=3):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = ChebConv(in_channels, 64, K)\n",
        "        self.conv2 = ChebConv(64, out_channels, K)\n",
        "        self.centroids = None  # To store class centroids\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "    # Function to calculate class centroids\n",
        "    def compute_class_centroids(self, x, y, mask):\n",
        "        \"\"\" Calculate centroids for each class based on the labeled data. \"\"\"\n",
        "        num_classes = y.max().item() + 1\n",
        "        centroids = []\n",
        "        for c in range(num_classes):\n",
        "            class_mask = (y[mask] == c)\n",
        "            class_embeddings = x[mask][class_mask]\n",
        "            if class_embeddings.size(0) > 0:\n",
        "                centroid = class_embeddings.mean(dim=0)\n",
        "            else:\n",
        "                centroid = torch.zeros(x.size(1)).to(x.device)  # Corrected to ensure it's on the right device\n",
        "            centroids.append(centroid)\n",
        "        self.centroids = torch.stack(centroids)\n",
        "\n",
        "    # Metric loss calculation\n",
        "    def metric_loss(self, h, y, mask):\n",
        "        \"\"\" Implement metric loss (if applicable). \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        # Example metric loss: cosine distance between embeddings and centroids\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        pos_centroids = self.centroids[y_masked]\n",
        "        pos_distances = torch.norm(h_masked - pos_centroids, dim=1)\n",
        "\n",
        "        return pos_distances.mean()\n",
        "\n",
        "    # Triplet loss calculation\n",
        "    def triplet_loss(self, h, y, mask, margin=1.0):\n",
        "        \"\"\" Triplet loss implementation with semi-hard negative mining. \"\"\"\n",
        "        if self.centroids is None:\n",
        "            return torch.tensor(0.0)  # Return a tensor to avoid gradient issues\n",
        "\n",
        "        h_masked = h[mask]\n",
        "        y_masked = y[mask]\n",
        "\n",
        "        # Calculate positive distances\n",
        "        pos_distances = torch.norm(h_masked - self.centroids[y_masked], dim=1)\n",
        "\n",
        "        # Calculate negative distances for all other classes\n",
        "        neg_distances = torch.cat([torch.norm(h_masked - self.centroids[i], dim=1).unsqueeze(1)\n",
        "                                   for i in range(self.centroids.size(0))], dim=1)\n",
        "\n",
        "        # Expand positive distances to match the shape of neg_distances\n",
        "        pos_distances_expanded = pos_distances.unsqueeze(1).expand_as(neg_distances)\n",
        "\n",
        "        # Semi-hard negative mining: select the negatives\n",
        "        semi_hard_negatives = torch.full_like(pos_distances, fill_value=margin + 1).to(h.device)  # Ensure it's on the correct device\n",
        "        for i in range(h_masked.size(0)):\n",
        "            neg_candidates = neg_distances[i][neg_distances[i] > pos_distances[i]]  # Valid negatives\n",
        "            if len(neg_candidates) > 0:\n",
        "                semi_hard_negatives[i] = neg_candidates.min()\n",
        "\n",
        "        # Calculate triplet loss (margin-based)\n",
        "        triplet_loss = F.relu(pos_distances - semi_hard_negatives + margin)\n",
        "\n",
        "        return triplet_loss.mean()\n",
        "\n",
        "# Step 5: Train and evaluate the model with combined loss (CE + Metric + Triplet)\n",
        "def train_model(data, model, epochs=200, lr=0.01):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        out = model(data)\n",
        "\n",
        "        # Compute class centroids for labeled data\n",
        "        model.compute_class_centroids(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Compute losses\n",
        "        ce_loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "        metric_loss = model.metric_loss(data.x, data.y, data.train_mask)\n",
        "        triplet_loss = model.triplet_loss(data.x, data.y, data.train_mask)\n",
        "\n",
        "        # Combine Cross-Entropy loss with Metric loss and Triplet loss\n",
        "        loss = ce_loss + 0.1 * metric_loss + 0.1 * triplet_loss  # Adjust the weight of each loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}, CE Loss: {ce_loss.item()}, Metric Loss: {metric_loss.item()}, Triplet Loss: {triplet_loss.item()}')\n",
        "\n",
        "def test_model(data, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data)\n",
        "        _, pred = out.max(dim=1)\n",
        "\n",
        "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "    acc = int(correct) / int(data.test_mask.sum())\n",
        "    print(f'Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Calculate AA and Kappa\n",
        "    AA, kappa = calculate_metrics(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    print(f'Average Accuracy (AA): {AA:.4f}, Kappa: {kappa:.4f}')\n",
        "\n",
        "    # Affichage de la précision par classe\n",
        "    class_precisions = calculate_class_precisions(data.y[data.test_mask].cpu().numpy(), pred[data.test_mask].cpu().numpy())\n",
        "    for class_index, precision in enumerate(class_precisions):\n",
        "        print(f'Precision for class {class_index}: {precision:.4f}')\n",
        "\n",
        "def calculate_class_precisions(y_true, y_pred):\n",
        "    \"\"\" Calculate precision for each class. \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_precisions = conf_matrix.diagonal() / conf_matrix.sum(axis=0)\n",
        "    return np.nan_to_num(class_precisions)  # Replace NaN with 0\n",
        "\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\" Calculate Average Accuracy (AA) and Cohen's Kappa. \"\"\"\n",
        "    # Calculate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "\n",
        "    # Average Accuracy (AA)\n",
        "    AA = np.nanmean(class_acc)  # Use nanmean to ignore any NaN values\n",
        "    kappa = cohen_kappa_score(y_true, y_pred)\n",
        "\n",
        "    return AA, kappa\n",
        "\n",
        "# Putting it all together\n",
        "def main():\n",
        "    hsi_data, gt_data = load_data()\n",
        "    x, y = preprocess_data(hsi_data, gt_data)\n",
        "\n",
        "    # Generate edge index using k-nearest neighbors and graph metrics\n",
        "    edge_index = generate_graph(x)\n",
        "\n",
        "    # Create train and test masks\n",
        "    train_mask, test_mask = split_data(y)\n",
        "\n",
        "    data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, test_mask=test_mask)\n",
        "\n",
        "    model = GCN(in_channels=x.size(1), out_channels=y.max().item() + 1)\n",
        "    train_model(data, model)\n",
        "    test_model(data, model)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lXfdAQP4gu-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}